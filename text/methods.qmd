# Methods {#sec-methods}

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "..")
`%>%` <- magrittr::`%>%`
```

> To empirically examine whether adaptation discourse reflects epistemological diversity or monoculture, I develop a methodological approach centered around the "Dominance Index"â€”a measurement tool for quantifying the degree to which adaptation discourse is concentrated around particular topics or perspectives.

The entire analysis was conducted in R [@base], with reproducible research principles supported by `renv` [@renv] for package management and Quarto for dynamic document generation. Data manipulation and transformation relied on the tidyverse ecosystem [@tidyverse], while the analytical pipeline employed custom functions designed for transparency and reproducibility. All code and data processing steps are documented and available for replication.

This approach was chosen for two reasons, first, open source software is free to use and second, these tools are developed, maintained and used by academics the world over. Since the packages are publically available, and a part of a peer-review process through the central R repository, they are quite stable and made for rigours analysis. This makes it possible for anyone to access the files I have developed for the project on [GitHub](https://github.com/nilsnerhus/oppgave), and validate my findings if they so choose. My findings are not, as often is in computational analysis, "hidden" on my computer, but available to all. 

The framework I am using here has been under intense development for the last four months, as I started without any prior knowledge of R, beyond simple calculations I did in my undergrad. The framework has slowly been written out, tested and rewritten, with the last major changes happening just days before the deadline. This has improved its efficiency and its reliability.

The framework is designed to be able to take any country-specific dataset, and run the same analysis with the same functions and assumptions, making it possible to in the future compare across policy areas. That kind of analysis is beyond the scope of this thesis, however.

The packages used in the analysis will be cited, and their documentation has been a central source of knowledge. An important note is that while this analysis is unsupervised machine learning, and thus is based on some of the principles that GAI-models are, they share no more similarity other than that they both use text to make inference. 

The framework for text analysis has three main stages. First, I download and prepare the texts, to clean them so that they become comparable. Then, I model the corpus into a set of topics we later can use for analysis, before I calculate the discourse centralization through the Dominance Index, and test it statistically. I use the STM package [@stm] for the calculation of everything except the Dominance Index, as that is my contribution.

## Corpus Collection and Preparation

> To level the playing field and make the NAPs comparable across documents, I have developed a systematic approach to extract, clean, and prepare the text, as well as finding and adding the necesarry metadata for analysis.

```{r}
#| label: Corpus cleaning
#| include: FALSE

## Load scripts
source("scripts/utils.R")
source("scripts/scrape_web.R")
source("scripts/extract_pdfs.R")
source("scripts/get_un_classifications.R")
source("scripts/add_metadata.R")
source("scripts/process_dfm.R")

## Set parameters
category_map <- list(
  Global = "global_category",    
  Income = "wb_income_level", 
  Region = "region", 
  Geography = c("is_sids", "is_lldc"),
  Time = "time_period"
)
time_groups <- c("Early" = 2018, "Middle" = 2021, "Late" = Inf)

## Run functions
web <- web_cache(scrape_web)
docs <- auto_cache(extract_pdfs, web)

un_classifications <- auto_cache(get_un_classifications)
metadata <- auto_cache(add_metadata, web, un_classifications, time = time_groups)

dfm <- auto_cache(process_dfm, docs, metadata, min_docs = 0.1, max_docs = 0.8)
```
```{r}
#| label: Extract values for corpus cleaning
#| include: FALSE

# Extract values
## Genral values
all_language_naps <- num(web[["metadata"]][["row_count"]])
english_docs <- num(web[["metadata"]][["document_count"]])

## Text cleaning values
init_tokens <- num(dfm[["metadata"]][["input_tokens"]])
final_tokens <- num(dfm[["metadata"]][["final_tokens"]])
init_terms <- num(dfm[["metadata"]][["input_vocabulary"]])
final_terms <- num(dfm[["metadata"]][["final_vocabulary"]])

lower_thresh <- pct(dfm[["metadata"]][["thresholds"]][["min_docs_prop"]])
upper_thresh <- pct(dfm[["metadata"]][["thresholds"]][["max_docs_prop"]])

## Metadata values
start_year <- num(min(metadata$data$metadata$year, na.rm=TRUE))
end_year <- num(max(metadata$data$metadata$year, na.rm=TRUE))

num_sids <- num(sum(metadata$data$metadata$is_sids, na.rm=TRUE))
num_lldc <- num(sum(metadata$data$metadata$is_lldc, na.rm=TRUE))
```

The analysis begins with the systematic collection of National Adaptation Plans from the UNFCCC's NAP Central website. To do this, I set up a web scraper using the `polite` package [@polite] that automatically scans the website and extracts the country name, the date posted and the link to the download for English language NAPs. I use `polite` to avoid overwhelming the website, and following their established rules for scraping. This process found `r english_docs` English plans, of `r all_language_naps` available countries. 

While focusing on English-language documents introduces a potential bias toward English-speaking countries or those with stronger ties to international institutions, this constraint was necessary to ensure meaningful textual comparison using consistent analytical methods. It is also valuable, as the working language of the World Bank and other international institutions is English [@defrancesco2020]. 

After scraping the web, I split the metadata from the documents. The documents were downloaded automatically, before the text was extracted using `pdftools` [@pdftools] and read into R. The text was then cleaned to make the comparable across the whole corpus using `quanteda` [@quanteda]. I used the default pipeline for text processing with the `stm` package [@stm]. This included removing common words with little semantic value known as stop words using `stopwords` [@stopwords], numbers, punctuation, and reducing all words to their stem using `SnowballC` [@SnowballC] removing their ending, so the same word with different endings is counted as one. 

The NAPs presented two text quality issues, first, as the documents are published as Portable Document Files (PDFs), a good format for printing, also introduces a lot of different artifacts when extracting the text. Second, the texts included national specific content like country-names and often a summary in the national language. Both cluttered the corpus and made the topics very low quality.

 To combat this, I filtered the corpus quite aggressively, removing words that occurred less than `r lower_thresh` of documents, and words that occurred in more than  `r upper_thresh` documents [@roberts2019].

The other issue was solved by making a list of stop words based on country names in the `countrycode` packages [@countrycode]. The whole process left us with `r final_tokens` of the `r init_tokens` words we started with, and `r final_terms` of `r init_terms` unique terms. 

The metadata was extracted and processed independently of the documents. The country name was extracted and standardized with the help of the `countrycode` package [@countrycode], that matched the country names to a three letter standardized code. These standardized names were then compared with the `wbstats` package [@wbstats], and added in the World Banks data on income, classifying the countries as low income, lower middle income, upper middle income and high income countries.

The same process was used to assign the countries to their World Bank region. These seven regions are East Asia and Pacific, Europe and Central Asia, Latin America and Caribbean, Middle East and North Africa, North America, South Asia, and Sub-Saharan Africa. Assigning these categories serves two functions, first it makes the country groups large enough for robust calculations, and we are using the largest development actors own definitions, to see if they have any impact on the content.

In addition to these categories, I scraped the website for the Office of the High Commissioner for Least Developed Countries, Landlocked Developing Countries and Small Islands Developing States using `polite` [@polite] and for web scraping. Here, I extracted a list of all countries classified as landlocked (LLDC) or as small islands (SIDS). These are proxies for countries facing the same geographic issues, across regions and income levels. This identified `r num_sids` SIDS and `r num_lldc`. 

The last category I extract is the date the plans were posted. The plans have been drafted since `r start_year`, and the latest in `r end_year`. To make it possible to see if the timing of the plans influence them, I split them into three groups, early (-2018), middle (until 2022) and late (2023-). Together, these categories allow us to examine whether discourse patterns align more strongly with economic positioning, regional institutions, or shared geographic vulnerabilities.

## Structural Topic Modeling

> Structural topic modeling allows us to discover latent themes in the corpus, reducing the corpus to word clusters we can analyze later. 

```{r}
#| label: Topic modeling
#| include: FALSE

## Load scripts
source("scripts/find_k.R")
source("scripts/fit_model.R")

# Run functions
k <- auto_cache(find_k, dfm)
model <- auto_cache(fit_model, dfm, k, category_map = category_map)

```
```{r}
#| label: Extract values from topic modeling
#| include: FALSE
# Extract values
k_value <- num(k[["data"]][["best_k"]])
coh_weight <- pct(k[["metadata"]][["weights"]][["coherence"]])
exc_weight <- pct(k[["metadata"]][["weights"]][["exclusivity"]])
iterations_run <- num(model[["metadata"]][["iterations_run"]])

segments <- num(dfm[["metadata"]][["segmentation"]][["segment_count"]])
```

Topic modelling is an approach that discovers thematic patterns in large text corpora in an unsupervised way. For our purposes, this makes it possible to interact with the whole corpus that spans  `r init_tokens` words, in an analytical way. This method was chosen, because unlike simple word frequency analysis or manual coding, topic models identify clusters of words that tend to co-occur across documents, revealing underlying themes that structure the discourse. 

The main assumption here is that each document contains a mix of topics, where topics are the likelihood for words to occur in the same document [@roberts2016]. For example, a topic related to agricultural adaptation might have high probabilities for words like "crop," "drought," "irrigation," and "yield," while a topic about coastal adaptation might emphasize "sea-level," "erosion," "storm," and "infrastructure." This means the model treats documents as "bags of words," meaning word order is not considered. This simplification allows for computationally efficient discovery of thematic patterns across our documents, but also loses sentence, paragraph or chapter-level semantics [@roberts2019]

One important note to this approach is the document-centric nature of it. It treats the document as the organizing principle, making corpuses with a few long documents like the NAPs centralized into a pattern of one topic per document. To avoid this, I automatically split the NAPs into `r segments` different segments, making the model run them as if they are separate documents, before I aggregate them again. This leads to considerably less concentration in each topic, raising the quality of them, but also introduces the risk of losing some content, since the documents are split after a certain amount of words. 

The innovation in structural topic modeling (STM) [@stm] is that it makes it possible for the model to investigate the relationship between the text corpus and the metadata for the documents [@roberts2019].  This is perfect for my research as we can pass the income level, region, and geographic characteristics, to the model.

Deciding on the right number of topics is done by one weighing the interpretability of a few topics, their semantic coherence, against making sure that a certain word is in as few topics as possible, their exclusivity. I used the `stm` package [@stm] to run a smaller model on a wide range of values, and weighing the coherence and exclusivity `r coh_weight` against `r exc_weight`, giving us a `r k_value` as the optimal number of topics. 

The full model was then run using spectral decomposition, which provides more stable and reproducible results compared to random [@roberts2019]. After running  `r iterations_run` iterations, the model converged successfully, indicating that the algorithm had identified stable topic distributions. These `r k_value` topics are the basis for the rest of my analysis. 

## Dominance analysis

> To analyse the distribution of topics I create a new measure, the Dominance Index, to investigate how concentrated the discourse is in a few topic. 

```{r}
#| label: Dominance analysis
#| include: FALSE

## Load scripts
source("scripts/name_topics.R")
source("scripts/find_dominance.R")
source("scripts/find_variance.R")
source("scripts/calculate_metrics.R")

# Set parameters
context <- " Follow these rules: Each label must be completely unique and do not 
name a topic after a country or city. These are National Adaptation Plan documents 
from the UNFCCC covering climate adaptation strategies. 

Earlier analysis has found the themes to be either security related (disasters or risks etc.), 
geographical (rangeland or coastal etc.) or sectoral (agriculture, fisheries, tourism)."

# Run functions
topics <- auto_cache(name_topics, model, context = context)
metrics <- auto_cache(calculate_metrics, model, topics, dfm, overwrite = TRUE)
```
```{r}  
#| label: Extract values from dominance analysis
#| include: FALSE
# Extract values
n_topics <- num(metrics[["metadata"]][["n_value"]])
```

For this analysis, I use two of the topic models outputs: The topic terms and the topic distributions. The topic terms are what the model deems to be the most frequent and exclusive terms for a topic. As these terms are stemmed, I used the `topiclabels` package [@topiclabels] to create readable labels. This made it possible to work more rapidly, as I did not have to re-analyze the topics manually every time I changed something earlier in the model, as well as removing my own biases. 

The topic distributions all sum to one, and identifies the share of the overall corpus that one topic is responsible for. This topic share is also what I use to calculate the Dominance Index, where I for each category, as we established earlier, I identify all countries belonging to a specific group. 

Then I use the topic shares to create an average topic distribution across all topics within that category, before I calculate the share of the top `r n_topics` topics within the category. This captures how much of each group's adaptation discourse that is within this smaller number of topics. 

To make sure the number of topics, here  `r k_value` and the number of top topics `r n_topics` does not influence the values, I normalize all values, making the baseline 0, and the max score 1. A high dominance score (approaching 1.0) indicates that most of the group's adaptation discourse focuses on just `r n_topics` topics, suggesting a concentrated or potentially constrained understanding of adaptation challenges and responses. 

For the high-level categories, that being income, region and geography, I calculate their values by simply averaging the values of their subcategories. That means that the income-level dominance value is the average of the dominance values of low income, lower middle income, higher middle income and high income countries. 

A lower dominance score, however, indicates more distributed attention across the full range of adaptation topics, potentially reflecting greater diversity. Importantly, the dominance measure is scale-independent, making it comparable across groups like small island developing states and large regional groupings.

While the Dominance Index reveals patterns of discourse concentration, establishing whether these patterns are statistically meaningful requires formal hypothesis testing. I employ structural topic modeling's `estimateEffect` function [@stm] to test whether categorical group membership significantly influences topic distributions.

For each group, I test the null hypothesis that group membership has no effect on the prominence of the dominant topics. The alternative hypothesis is that group membership significantly influences these topic shares, suggesting that the categories (income level, regional context, or geographic vulnerability) systematically shape how countries write their national adaptation plans.

The statistical test works by fitting regression models for each dominant topic, with group membership as the predictor variable and topic proportions as the outcome. I use binary indicators for each group (for example, SIDS vs. non-SIDS, or low-income vs. other income levels) to test specific hypotheses about discourse patterns. 

I used the standard statistical significance of in the STM package [@stm] on 95%. I store the p-values as a way to analyse the relative power of the countries in each group. Lower p-values indicate stronger evidence that group membership systematically shapes discourse patterns